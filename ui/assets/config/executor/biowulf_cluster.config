// Config for https://hpc.nih.gov/
// This config doesn't use profiles because EGAPx can't use them now.
// It is a copy of 'biowulf' profile from config at https://hpc.nih.gov/apps/nextflow.html

params {
  config_profile_description = 'Biowulf nf-core config'
  config_profile_contact = 'staff@hpc.nih.gov'
  config_profile_url = 'https://hpc.nih.gov/apps/nextflow.html'
  max_memory = '224 GB'
  max_cpus = 32
  max_time = '72 h'

  igenomes_base = '/fdb/igenomes_nf/'
}


singularity {
    enabled = true
    autoMounts = true
    cacheDir = "/data/$USER/singularity"
    envWhitelist='https_proxy,http_proxy,ftp_proxy,DISPLAY,SLURM_JOB_ID,SINGULARITY_BINDPATH'
}

env {
    SINGULARITY_CACHEDIR="/data/$USER/singularity"
    PYTHONNOUSERSITE = 1
    DEBUG_STACK_TRACE_LEVEL="Warning"
    EXCEPTION_STACK_TRACE_LEVEL="Warning"
    DIAG_POST_LEVEL="Trace"
    DEBUG_CATCH_UNHANDLED_EXCEPTIONS="0"
    DEBUGINFOD_URLS="/dev/null"

// these turn off connecting to local NCBI services, it can simulate running without certain network access
// CONN_DISPD_DISABLE="1"
// CONN_LBSMD_DISABLE="1"
}

process {
    executor = 'slurm'
    maxRetries = 1
    queue = 'norm'
    queueSize = 200
    pollInterval = '2 min'
    queueStatInterval = '5 min'
    submitRateLimit = '6/1min'
    retry.maxAttempts = 1

    clusterOptions = ' --gres=lscratch:200 '

    scratch = '/lscratch/$SLURM_JOB_ID'
    // with the default stageIn and stageOut settings using scratch can
    // result in humungous work folders
    // see https://github.com/nextflow-io/nextflow/issues/961 and
    //     https://www.nextflow.io/docs/latest/process.html?highlight=stageinmode
    stageInMode = 'symlink'
    stageOutMode = 'rsync'
    
    // for running pipeline on group sharing data directory, this can avoid inconsistent files timestamps
    cache = 'lenient'
}
